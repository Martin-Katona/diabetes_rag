{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics (from llm_similarity_results_singlerun/mistral_evaluation_results.json):\n",
      "  Accuracy: 0.9900\n",
      "  Precision: 0.9804\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 0.9901\n",
      "  True Positives (TP): 100\n",
      "  True Negatives (TN): 98\n",
      "  False Positives (FP): 2\n",
      "  False Negatives (FN): 0\n",
      "Metrics for mistral: {'accuracy': 0.99, 'precision': 0.9803921568627451, 'recall': 1.0, 'f1': 0.9900990099009901, 'tp': 100, 'tn': 98, 'fp': 2, 'fn': 0}\n",
      "Evaluation Metrics (from llm_similarity_results_singlerun/llama3_evaluation_results.json):\n",
      "  Accuracy: 0.9350\n",
      "  Precision: 0.8850\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 0.9390\n",
      "  True Positives (TP): 100\n",
      "  True Negatives (TN): 87\n",
      "  False Positives (FP): 13\n",
      "  False Negatives (FN): 0\n",
      "Metrics for llama3: {'accuracy': 0.935, 'precision': 0.8849557522123894, 'recall': 1.0, 'f1': 0.9389671361502347, 'tp': 100, 'tn': 87, 'fp': 13, 'fn': 0}\n",
      "Evaluation Metrics (from llm_similarity_results_singlerun/gemma2:2b_evaluation_results.json):\n",
      "  Accuracy: 0.9700\n",
      "  Precision: 0.9434\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 0.9709\n",
      "  True Positives (TP): 100\n",
      "  True Negatives (TN): 94\n",
      "  False Positives (FP): 6\n",
      "  False Negatives (FN): 0\n",
      "Metrics for gemma2:2b: {'accuracy': 0.97, 'precision': 0.9433962264150944, 'recall': 1.0, 'f1': 0.970873786407767, 'tp': 100, 'tn': 94, 'fp': 6, 'fn': 0}\n",
      "Evaluation Metrics (from llm_similarity_results_singlerun/gemma2:9b_evaluation_results.json):\n",
      "  Accuracy: 0.9950\n",
      "  Precision: 0.9901\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 0.9950\n",
      "  True Positives (TP): 100\n",
      "  True Negatives (TN): 99\n",
      "  False Positives (FP): 1\n",
      "  False Negatives (FN): 0\n",
      "Metrics for gemma2:9b: {'accuracy': 0.995, 'precision': 0.9900990099009901, 'recall': 1.0, 'f1': 0.9950248756218906, 'tp': 100, 'tn': 99, 'fp': 1, 'fn': 0}\n",
      "Evaluation Metrics (from llm_similarity_results_singlerun/qwen2:1.5b_evaluation_results.json):\n",
      "  Accuracy: 0.9600\n",
      "  Precision: 0.9694\n",
      "  Recall: 0.9500\n",
      "  F1 Score: 0.9596\n",
      "  True Positives (TP): 95\n",
      "  True Negatives (TN): 97\n",
      "  False Positives (FP): 3\n",
      "  False Negatives (FN): 5\n",
      "Metrics for qwen2:1.5b: {'accuracy': 0.96, 'precision': 0.9693877551020408, 'recall': 0.95, 'f1': 0.9595959595959596, 'tp': 95, 'tn': 97, 'fp': 3, 'fn': 5}\n",
      "Evaluation Metrics (from llm_similarity_results_singlerun/qwen2:7b_evaluation_results.json):\n",
      "  Accuracy: 0.9850\n",
      "  Precision: 0.9899\n",
      "  Recall: 0.9800\n",
      "  F1 Score: 0.9849\n",
      "  True Positives (TP): 98\n",
      "  True Negatives (TN): 99\n",
      "  False Positives (FP): 1\n",
      "  False Negatives (FN): 2\n",
      "Metrics for qwen2:7b: {'accuracy': 0.985, 'precision': 0.98989898989899, 'recall': 0.98, 'f1': 0.9849246231155779, 'tp': 98, 'tn': 99, 'fp': 1, 'fn': 2}\n",
      "Evaluation Metrics (from llm_similarity_results_singlerun/qwen2.5:0.5b_evaluation_results.json):\n",
      "  Accuracy: 0.6400\n",
      "  Precision: 0.9118\n",
      "  Recall: 0.3100\n",
      "  F1 Score: 0.4627\n",
      "  True Positives (TP): 31\n",
      "  True Negatives (TN): 97\n",
      "  False Positives (FP): 3\n",
      "  False Negatives (FN): 69\n",
      "Metrics for qwen2.5:0.5b: {'accuracy': 0.64, 'precision': 0.9117647058823529, 'recall': 0.31, 'f1': 0.4626865671641791, 'tp': 31, 'tn': 97, 'fp': 3, 'fn': 69}\n",
      "Evaluation Metrics (from llm_similarity_results_singlerun/qwen2.5:1.5b_evaluation_results.json):\n",
      "  Accuracy: 0.9850\n",
      "  Precision: 0.9709\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 0.9852\n",
      "  True Positives (TP): 100\n",
      "  True Negatives (TN): 97\n",
      "  False Positives (FP): 3\n",
      "  False Negatives (FN): 0\n",
      "Metrics for qwen2.5:1.5b: {'accuracy': 0.985, 'precision': 0.970873786407767, 'recall': 1.0, 'f1': 0.9852216748768473, 'tp': 100, 'tn': 97, 'fp': 3, 'fn': 0}\n",
      "Evaluation Metrics (from llm_similarity_results_singlerun/zephyr:7b_evaluation_results.json):\n",
      "  Accuracy: 0.9050\n",
      "  Precision: 0.8462\n",
      "  Recall: 0.9900\n",
      "  F1 Score: 0.9124\n",
      "  True Positives (TP): 99\n",
      "  True Negatives (TN): 82\n",
      "  False Positives (FP): 18\n",
      "  False Negatives (FN): 1\n",
      "Metrics for zephyr:7b: {'accuracy': 0.905, 'precision': 0.8461538461538461, 'recall': 0.99, 'f1': 0.9124423963133641, 'tp': 99, 'tn': 82, 'fp': 18, 'fn': 1}\n",
      "Evaluation Metrics (from llm_similarity_results_singlerun/deepseek-r1:1.5b_evaluation_results.json):\n",
      "  Accuracy: 0.7350\n",
      "  Precision: 0.6741\n",
      "  Recall: 0.9100\n",
      "  F1 Score: 0.7745\n",
      "  True Positives (TP): 91\n",
      "  True Negatives (TN): 56\n",
      "  False Positives (FP): 44\n",
      "  False Negatives (FN): 9\n",
      "Metrics for deepseek-r1:1.5b: {'accuracy': 0.735, 'precision': 0.674074074074074, 'recall': 0.91, 'f1': 0.774468085106383, 'tp': 91, 'tn': 56, 'fp': 44, 'fn': 9}\n",
      "Evaluation Metrics (from llm_similarity_results_singlerun/deepseek-r1:7b_evaluation_results.json):\n",
      "  Accuracy: 0.9150\n",
      "  Precision: 0.8609\n",
      "  Recall: 0.9900\n",
      "  F1 Score: 0.9209\n",
      "  True Positives (TP): 99\n",
      "  True Negatives (TN): 84\n",
      "  False Positives (FP): 16\n",
      "  False Negatives (FN): 1\n",
      "Metrics for deepseek-r1:7b: {'accuracy': 0.915, 'precision': 0.8608695652173913, 'recall': 0.99, 'f1': 0.9209302325581395, 'tp': 99, 'tn': 84, 'fp': 16, 'fn': 1}\n",
      "Evaluation Metrics (from llm_similarity_results_singlerun/phi3:3.8b_evaluation_results.json):\n",
      "  Accuracy: 0.8700\n",
      "  Precision: 0.7984\n",
      "  Recall: 0.9900\n",
      "  F1 Score: 0.8839\n",
      "  True Positives (TP): 99\n",
      "  True Negatives (TN): 75\n",
      "  False Positives (FP): 25\n",
      "  False Negatives (FN): 1\n",
      "Metrics for phi3:3.8b: {'accuracy': 0.87, 'precision': 0.7983870967741935, 'recall': 0.99, 'f1': 0.8839285714285714, 'tp': 99, 'tn': 75, 'fp': 25, 'fn': 1}\n",
      "Evaluation Metrics (from llm_similarity_results_singlerun/granite3-moe:1b_evaluation_results.json):\n",
      "  Accuracy: 0.5900\n",
      "  Precision: 0.5506\n",
      "  Recall: 0.9800\n",
      "  F1 Score: 0.7050\n",
      "  True Positives (TP): 98\n",
      "  True Negatives (TN): 20\n",
      "  False Positives (FP): 80\n",
      "  False Negatives (FN): 2\n",
      "Metrics for granite3-moe:1b: {'accuracy': 0.59, 'precision': 0.550561797752809, 'recall': 0.98, 'f1': 0.7050359712230215, 'tp': 98, 'tn': 20, 'fp': 80, 'fn': 2}\n",
      "Evaluation Metrics (from llm_similarity_results_singlerun/granite3-moe:3b_evaluation_results.json):\n",
      "  Accuracy: 0.8850\n",
      "  Precision: 0.8130\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 0.8969\n",
      "  True Positives (TP): 100\n",
      "  True Negatives (TN): 77\n",
      "  False Positives (FP): 23\n",
      "  False Negatives (FN): 0\n",
      "Metrics for granite3-moe:3b: {'accuracy': 0.885, 'precision': 0.8130081300813008, 'recall': 1.0, 'f1': 0.8968609865470852, 'tp': 100, 'tn': 77, 'fp': 23, 'fn': 0}\n"
     ]
    }
   ],
   "source": [
    "#LLM context similarity testing\n",
    "\n",
    "from rag_evaluation import evaluate_llm_similarity,evaluate_results_file\n",
    "# List of models to evaluate\n",
    "models = [\n",
    "    \"mistral\",\n",
    "    \"llama3\",\n",
    "    \"gemma2:2b\",\n",
    "    \"gemma2:9b\",\n",
    "    \"qwen2:1.5b\",\n",
    "    \"qwen2:7b\",\n",
    "    \"qwen2.5:0.5b\",\n",
    "    \"qwen2.5:1.5b\",\n",
    "    \"zephyr:7b\",\n",
    "    \"deepseek-r1:1.5b\",\n",
    "    \"deepseek-r1:7b\",\n",
    "    \"phi3:3.8b\",\n",
    "    \"granite3-moe:1b\",\n",
    "    \"granite3-moe:3b\"\n",
    "]\n",
    "\n",
    "\n",
    "# Loop through each model\n",
    "for model_name in models:\n",
    "    # Define the output file path\n",
    "    output_file = f\"llm_similarity_results/{model_name}_evaluation_results.json\"\n",
    "    \n",
    "    # Evaluate LLM similarity\n",
    "    evaluate_llm_similarity(\"similarity_ds.json\", model_name=model_name, output_file=output_file)\n",
    "    \n",
    "    # Evaluate the results file and get metrics\n",
    "    metrics = evaluate_results_file(output_file)\n",
    "    \n",
    "    # Print or log the metrics for the current model\n",
    "    print(f\"Metrics for {model_name}: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: mistral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: llama3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: gemma2:2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: gemma2:9b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: qwen2:1.5b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: qwen2:7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: qwen2.5:0.5b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: qwen2.5:1.5b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: zephyr:7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: deepseek-r1:1.5b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: deepseek-r1:7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: phi3:3.8b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: granite3-moe:1b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: granite3-moe:3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: mistral, Accuracy: 0.97%\n",
      "Model: llama3, Accuracy: 0.99%\n",
      "Model: gemma2:2b, Accuracy: 0.95%\n",
      "Model: gemma2:9b, Accuracy: 0.98%\n",
      "Model: qwen2:1.5b, Accuracy: 0.94%\n",
      "Model: qwen2:7b, Accuracy: 0.98%\n",
      "Model: qwen2.5:0.5b, Accuracy: 0.87%\n",
      "Model: qwen2.5:1.5b, Accuracy: 0.94%\n",
      "Model: zephyr:7b, Accuracy: 0.99%\n",
      "Model: deepseek-r1:1.5b, Accuracy: 0.90%\n",
      "Model: deepseek-r1:7b, Accuracy: 0.98%\n",
      "Model: phi3:3.8b, Accuracy: 0.94%\n",
      "Model: granite3-moe:1b, Accuracy: 0.77%\n",
      "Model: granite3-moe:3b, Accuracy: 0.86%\n",
      "\n",
      "Sorted by accuracy:\n",
      "RAG Model: llama3, Accuracy: 0.99%\n",
      "RAG Model: zephyr:7b, Accuracy: 0.99%\n",
      "RAG Model: gemma2:9b, Accuracy: 0.98%\n",
      "RAG Model: qwen2:7b, Accuracy: 0.98%\n",
      "RAG Model: deepseek-r1:7b, Accuracy: 0.98%\n",
      "RAG Model: mistral, Accuracy: 0.97%\n",
      "RAG Model: gemma2:2b, Accuracy: 0.95%\n",
      "RAG Model: qwen2:1.5b, Accuracy: 0.94%\n",
      "RAG Model: qwen2.5:1.5b, Accuracy: 0.94%\n",
      "RAG Model: phi3:3.8b, Accuracy: 0.94%\n",
      "RAG Model: deepseek-r1:1.5b, Accuracy: 0.90%\n",
      "RAG Model: qwen2.5:0.5b, Accuracy: 0.87%\n",
      "RAG Model: granite3-moe:3b, Accuracy: 0.86%\n",
      "RAG Model: granite3-moe:1b, Accuracy: 0.77%\n"
     ]
    }
   ],
   "source": [
    "# RAG evaluation by single EVAL_LLM for every RAG_LLM\n",
    "\n",
    "from rag_evaluation import evaluate_questions,calculate_accuracy\n",
    "\n",
    "\n",
    "rag_models = [\n",
    "    \"mistral\",\n",
    "    \"llama3\",\n",
    "    \"gemma2:2b\",\n",
    "    \"gemma2:9b\",\n",
    "    \"qwen2:1.5b\",\n",
    "    \"qwen2:7b\",\n",
    "    \"qwen2.5:0.5b\",\n",
    "    \"qwen2.5:1.5b\",\n",
    "    \"zephyr:7b\",\n",
    "    \"deepseek-r1:1.5b\",\n",
    "    \"deepseek-r1:7b\",\n",
    "    \"phi3:3.8b\",\n",
    "    \"granite3-moe:1b\",\n",
    "    \"granite3-moe:3b\"\n",
    "]\n",
    "\n",
    "output_file = \"eval_results_rag_model_individual/evaluation_results_{}_{}.json\"\n",
    "json_file = \"eval_results_rag_model_3comb/evaluation_results_{}.json\"\n",
    "\n",
    "eval_model = \"qwen2.5:1.5b\"\n",
    "\n",
    "model_accuracies = {}\n",
    "for rag_model in rag_models:\n",
    "    print(f\"Evaluating model: {rag_model}\")\n",
    "    #evaluate_questions(\"rag_eval_ds.json\", output_file=output_file.format(rag_model), rag_model=rag_model,eval_models=['mistral','gemma2:9b','qwen2.5:1.5b'])\n",
    "    evaluate_questions(json_file=json_file.format(rag_model), output_file=output_file.format(eval_model,rag_model), rag_model=rag_model,eval_models=[eval_model])\n",
    "\n",
    "    accuracy = calculate_accuracy(output_file.format(eval_model,rag_model))\n",
    "    model_accuracies[rag_model] = accuracy\n",
    "\n",
    "for model, accuracy in model_accuracies.items():\n",
    "    print(f\"Model: {model}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "sorted_models = sorted(model_accuracies.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted by accuracy:\")\n",
    "for model, accuracy in sorted_models:\n",
    "    print(f\"RAG Model: {model}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single RAG_LLM evaluated by every EVAL_LLM\n",
    "from rag_evaluation import evaluate_questions,calculate_accuracy\n",
    "\n",
    "\n",
    "eval_models = [\n",
    "    \"mistral\",\n",
    "    \"llama3\",\n",
    "    \"gemma2:2b\",\n",
    "    \"gemma2:9b\",\n",
    "    \"qwen2:1.5b\",\n",
    "    \"qwen2:7b\",\n",
    "    \"qwen2.5:0.5b\",\n",
    "    \"qwen2.5:1.5b\",\n",
    "    \"zephyr:7b\",\n",
    "    \"deepseek-r1:1.5b\",\n",
    "    \"deepseek-r1:7b\",\n",
    "    \"phi3:3.8b\",\n",
    "    \"granite3-moe:1b\",\n",
    "    \"granite3-moe:3b\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "output_file = \"eval_results_eval_model/evaluation_results_{}.json\"\n",
    "\n",
    "model_accuracies = {}\n",
    "for eval_model in eval_models:\n",
    "    print(f\"Evaluating model: {eval_model}\")\n",
    "    evaluate_questions(\"rag_eval_ds.json\", output_file=output_file.format(eval_model), rag_model='mistral',eval_models=[eval_model])\n",
    "    accuracy = calculate_accuracy(output_file.format(eval_model))\n",
    "    model_accuracies[eval_model] = accuracy\n",
    "\n",
    "for model, accuracy in model_accuracies.items():\n",
    "    print(f\"Model: {model}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "sorted_models = sorted(model_accuracies.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted by accuracy:\")\n",
    "for model, accuracy in sorted_models:\n",
    "    print(f\"Eval Model: {model}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL - Every RAG_LLM evaluated by 3-comb.\n",
    "\n",
    "from rag_evaluation import evaluate_questions,calculate_accuracy\n",
    "\n",
    "\n",
    "rag_models = [\n",
    "    \"mistral\",\n",
    "    \"llama3\",\n",
    "    \"gemma2:2b\",\n",
    "    \"gemma2:9b\",\n",
    "    \"qwen2:1.5b\",\n",
    "    \"qwen2:7b\",\n",
    "    \"qwen2.5:0.5b\",\n",
    "    \"qwen2.5:1.5b\",\n",
    "    \"zephyr:7b\",\n",
    "    \"deepseek-r1:1.5b\",\n",
    "    \"deepseek-r1:7b\",\n",
    "    \"phi3:3.8b\",\n",
    "    \"granite3-moe:1b\",\n",
    "    \"granite3-moe:3b\"\n",
    "]\n",
    "\n",
    "\n",
    "output_file = \"eval_results_rag_model_3comb/evaluation_results_{}.json\"\n",
    "\n",
    "model_accuracies = {}\n",
    "for rag_model in rag_models:\n",
    "    print(f\"Evaluating model: {rag_model}\")\n",
    "    evaluate_questions(\"rag_eval_ds.json\", output_file=output_file.format(rag_model), rag_model=rag_model,eval_models=['mistral','gemma2:9b','qwen2.5:1.5b'])\n",
    "    #evaluate_questions(\"eval_results_nest/evaluation_results_qwen2.5:1.5b.json\", output_file=output_file.format(rag_model), rag_model=rag_model,eval_models=['mistral','gemma2:9b','qwen2.5:1.5b'])\n",
    "\n",
    "    accuracy = calculate_accuracy(output_file.format(rag_model))\n",
    "    model_accuracies[rag_model] = accuracy\n",
    "\n",
    "for model, accuracy in model_accuracies.items():\n",
    "    print(f\"Model: {model}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "sorted_models = sorted(model_accuracies.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nSorted by accuracy:\")\n",
    "for model, accuracy in sorted_models:\n",
    "    print(f\"RAG Model: {model}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mResponse from gemma2:9b: false\u001b[0m\n",
      "\u001b[91mResponse from gemma2:9b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from gemma2:9b: false\u001b[0m\n",
      "\u001b[91mResponse from mistral: false\u001b[0m\n",
      "\u001b[91mResponse from mistral: false\u001b[0m\n",
      "\u001b[91mResponse from gemma2:9b: false\u001b[0m\n",
      "\u001b[91mMajority voting result: False\u001b[0m\n",
      "\u001b[91mResponse from gemma2:9b: false\u001b[0m\n",
      "\u001b[91mResponse from mistral: false\u001b[0m\n",
      "\u001b[91mResponse from gemma2:9b: false\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mMajority voting result: False\u001b[0m\n",
      "\u001b[91mResponse from qwen2.5:1.5b: false\u001b[0m\n",
      "\u001b[91mResponse from mistral: false\u001b[0m\n",
      "\u001b[91mResponse from gemma2:9b: false\u001b[0m\n",
      "\u001b[91mResponse from gemma2:9b: false\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n"
     ]
    }
   ],
   "source": [
    "#RAG evaluation by 3-comb. for a single RAG_LLM \n",
    "\n",
    "from rag_evaluation import evaluate_questions,calculate_accuracy\n",
    "\n",
    "output_file = \"eval_results_rag_model_3comb/evaluation_results_gemma2:9b_mistral_qwen2.5:1.5b.json\"\n",
    "evaluate_questions(\"rag_eval_ds.json\", output_file=output_file, rag_model='mistral',eval_models=['mistral','gemma2:9b','qwen2.5:1.5b'])\n",
    "\n",
    "accuracy = calculate_accuracy(output_file)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
